<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JARVIS AI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Rajdhani:wght[400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --jarvis-blue: #00f3ff;
            --jarvis-dark: #0a192f;
            --jarvis-glow: 0 0 20px rgba(0, 243, 255, 0.6);
        }

        body {
            background-color: #050505;
            color: var(--jarvis-blue);
            font-family: 'Rajdhani', sans-serif;
            overflow: hidden;
            height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }

        /* Scanline effect */
        body::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(
                to bottom,
                rgba(0, 243, 255, 0.03) 50%,
                rgba(0, 0, 0, 0.1) 50%
            );
            background-size: 100% 4px;
            pointer-events: none;
            z-index: 10;
        }

        /* Vignette */
        body::after {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: radial-gradient(circle, transparent 60%, #000 100%);
            pointer-events: none;
            z-index: 9;
        }

        .orbitron {
            font-family: 'Orbitron', sans-serif;
        }

        /* Arc Reactor / Listening Animation */
        .reactor-container {
            position: relative;
            width: 200px;
            height: 200px;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: all 0.5s ease;
        }

        .reactor-ring {
            position: absolute;
            border-radius: 50%;
            border: 2px solid var(--jarvis-blue);
            box-shadow: var(--jarvis-glow);
            opacity: 0.8;
        }

        .ring-1 { width: 100%; height: 100%; animation: spin 10s linear infinite; border-bottom-color: transparent; border-right-color: transparent;}
        .ring-2 { width: 80%; height: 80%; animation: spin-reverse 5s linear infinite; border-top-color: transparent; border-left-color: transparent;}
        .ring-3 { width: 60%; height: 60%; animation: spin 3s linear infinite; border: 4px dashed var(--jarvis-blue);}
        
        .core {
            width: 30%;
            height: 30%;
            background: radial-gradient(circle, #fff 0%, var(--jarvis-blue) 60%, transparent 100%);
            border-radius: 50%;
            box-shadow: 0 0 40px var(--jarvis-blue);
            animation: pulse 2s ease-in-out infinite;
        }

        /* States */
        .state-idle .reactor-container {
            opacity: 0; 
            transform: scale(0.5);
            pointer-events: none;
            position: absolute; 
        }

        .state-active .reactor-container {
            opacity: 1;
            transform: scale(1.5);
        }

        .state-idle .clock-container {
            opacity: 1;
            transform: scale(1);
        }

        .state-active .clock-container {
            opacity: 0.3; /* Dim but visible */
            transform: scale(0.8);
            filter: blur(2px);
        }

        .clock-container {
            text-align: center;
            transition: all 0.5s ease;
            z-index: 20;
        }

        .time-display {
            font-size: 15vw;
            line-height: 1;
            text-shadow: 0 0 30px rgba(0, 243, 255, 0.4);
            font-weight: 700;
            letter-spacing: 0.05em;
        }

        .date-display {
            font-size: 3vw;
            letter-spacing: 0.5em;
            text-transform: uppercase;
            opacity: 0.8;
            margin-top: -10px;
        }

        /* Status Text */
        .status-text {
            position: absolute;
            bottom: 10%;
            font-size: 1.5rem;
            letter-spacing: 2px;
            text-transform: uppercase;
            animation: blink 1s infinite;
        }

        .init-btn {
            position: absolute;
            bottom: 20px;
            padding: 10px 30px;
            background: transparent;
            border: 1px solid var(--jarvis-blue);
            color: var(--jarvis-blue);
            font-family: 'Orbitron', sans-serif;
            text-transform: uppercase;
            cursor: pointer;
            transition: 0.3s;
            z-index: 50;
            letter-spacing: 2px;
        }

        .init-btn:hover {
            background: rgba(0, 243, 255, 0.1);
            box-shadow: 0 0 20px rgba(0, 243, 255, 0.4);
        }

        @keyframes spin { 100% { transform: rotate(360deg); } }
        @keyframes spin-reverse { 100% { transform: rotate(-360deg); } }
        @keyframes pulse { 0%, 100% { opacity: 0.5; transform: scale(0.9); } 50% { opacity: 1; transform: scale(1.1); } }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
        @keyframes waveform { 0% { height: 10px; } 50% { height: 50px; } 100% { height: 10px; } }

        /* Sound Wave for Speaking */
        .wave-container {
            display: none;
            gap: 5px;
            align-items: center;
            height: 60px;
        }
        .state-speaking .wave-container {
            display: flex;
        }
        .state-speaking .reactor-container {
            display: none;
        }
        .wave-bar {
            width: 6px;
            background: var(--jarvis-blue);
            animation: waveform 0.5s ease-in-out infinite;
        }
        .wave-bar:nth-child(1) { animation-duration: 0.4s; }
        .wave-bar:nth-child(2) { animation-duration: 0.6s; }
        .wave-bar:nth-child(3) { animation-duration: 0.3s; }
        .wave-bar:nth-child(4) { animation-duration: 0.5s; }
        .wave-bar:nth-child(5) { animation-duration: 0.4s; }

        .hidden { display: none; }
    </style>
</head>
<body class="state-idle">

    <!-- Clock Display (Idle State) -->
    <div class="clock-container orbitron">
        <div id="time" class="time-display">00:00</div>
        <div id="date" class="date-display">JANUARY 01 2025</div>
    </div>

    <!-- Processing/Listening Animation (Active State) -->
    <div class="reactor-container">
        <div class="reactor-ring ring-1"></div>
        <div class="reactor-ring ring-2"></div>
        <div class="reactor-ring ring-3"></div>
        <div class="core"></div>
    </div>

    <!-- Speaking Animation -->
    <div class="wave-container">
        <div class="wave-bar"></div>
        <div class="wave-bar"></div>
        <div class="wave-bar"></div>
        <div class="wave-bar"></div>
        <div class="wave-bar"></div>
    </div>

    <div id="status" class="status-text hidden">System Offline</div>

    <button id="initBtn" class="init-btn" onclick="initializeJarvis()">Initialize System</button>

    <script>
        // --- Configuration ---
        const WAKE_WORD = "jarvis";
        const apiKey = ""; // API Key will be injected by the environment
        
        // --- State Management ---
        let isListening = false;
        let isSpeaking = false;
        let isProcessing = false; // Flag for when AI is thinking/speaking
        let isStarting = false; // NEW FLAG: True when recognition.start() is called but onstart hasn't fired
        let isInitialized = false;
        let recognition = null;
        let synthesis = window.speechSynthesis;
        let voices = [];
        let restartTimer = null;
        let chatHistory = []; // Stores conversation history for context
        
        // --- DOM Elements ---
        const timeEl = document.getElementById('time');
        const dateEl = document.getElementById('date');
        const statusEl = document.getElementById('status');
        const bodyEl = document.body;
        const initBtn = document.getElementById('initBtn');

        // --- Clock Logic ---
        function updateClock() {
            const now = new Date();
            let hours = now.getHours();
            const minutes = String(now.getMinutes()).padStart(2, '0');
            const ampm = hours >= 12 ? '' : ''; 
            hours = String(hours).padStart(2, '0');
            
            timeEl.textContent = `${hours}:${minutes}`;
            
            const options = { weekday: 'long', month: 'long', day: 'numeric' };
            dateEl.textContent = now.toLocaleDateString('en-US', options).toUpperCase();
        }
        setInterval(updateClock, 1000);
        updateClock();

        // --- Initialization ---
        async function initializeJarvis() {
            if (isInitialized) return;
            isInitialized = true;

            initBtn.style.display = 'none';
            statusEl.classList.remove('hidden');
            statusEl.textContent = "INITIALIZING PROTOCOLS...";
            
            try {
                // Request Mic Permission via a dummy stream
                await navigator.mediaDevices.getUserMedia({ audio: true });
                setupSpeechRecognition();
                loadVoices();
                speak("System online. I am listening.");
                setStatus("Standing By");
            } catch (err) {
                console.error(err);
                setStatus("Mic Access Denied");
                // Using a custom message box instead of alert() for better UX
                const customAlert = document.createElement('div');
                customAlert.textContent = "Microphone access is required for Jarvis to function.";
                customAlert.style.cssText = "position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); background: #1a1a1a; border: 2px solid var(--jarvis-blue); padding: 20px; z-index: 100; color: var(--jarvis-blue); font-family: 'Orbitron', sans-serif;";
                document.body.appendChild(customAlert);
                setTimeout(() => document.body.removeChild(customAlert), 3000);

                initBtn.style.display = 'block';
                isInitialized = false;
            }
        }

        // --- Helper: Safe Start ---
        async function startRecognition() {
            // Only proceed if recognition object exists, we are not already listening, and we are not already trying to start
            if (!recognition || isListening || isStarting) {
                return;
            }
            
            isStarting = true;
            try {
                // Attempt to start
                await recognition.start();
            } catch (e) {
                isStarting = false; // Failed to start, reset flag
                // Ignore "already started" errors
                if (e.name !== 'InvalidStateError' && e.message !== 'recognition has already started') {
                    console.error("Error starting recognition:", e);
                }
            }
        }

        // --- Speech Recognition ---
        function setupSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            
            if (!SpeechRecognition) {
                // Using a custom message box instead of alert()
                const customAlert = document.createElement('div');
                customAlert.textContent = "Browser not supported. Please use Chrome or Edge.";
                customAlert.style.cssText = "position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); background: #1a1a1a; border: 2px solid var(--jarvis-blue); padding: 20px; z-index: 100; color: var(--jarvis-blue); font-family: 'Orbitron', sans-serif;";
                document.body.appendChild(customAlert);
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = true; // Keep listening
            recognition.interimResults = false; // We only want final commands
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                isListening = true;
                isStarting = false; // Successfully started
                console.log("Recognition started");
            };

            recognition.onend = () => {
                isListening = false;
                isStarting = false; // Recognition stopped, reset start flag
                console.log("Recognition ended");
                
                // Clear any existing restart timers to prevent stacking
                if (restartTimer) {
                    clearTimeout(restartTimer);
                    restartTimer = null;
                }

                // Auto-restart only if we are not busy speaking or processing (unintentional stop)
                if (!isSpeaking && !isProcessing) {
                    restartTimer = setTimeout(() => {
                        if (!isSpeaking && !isProcessing) {
                            startRecognition();
                        }
                    }, 500); // Reduced timeout
                }
                // If isSpeaking or isProcessing is true, restart is handled by processCommand's finally block
            };

            recognition.onresult = async (event) => {
                // If we are currently speaking, ignore input to avoid hearing ourselves
                if (isSpeaking) return;

                const lastResultIndex = event.results.length - 1;
                const transcript = event.results[lastResultIndex][0].transcript.trim().toLowerCase();
                
                console.log("Heard:", transcript);

                if (transcript.includes(WAKE_WORD)) {
                    // Visual feedback
                    setVisualState('active');
                    setStatus("Processing...");

                    // Extract command (everything after "jarvis")
                    const command = transcript.split(WAKE_WORD)[1].trim();

                    if (!command || command.length < 2) {
                        // User just said "Jarvis"
                        await speak("Yes?");
                    } else {
                        // Process the full command
                        await processCommand(command);
                    }
                }
            };

            // Call start, but let the internal guard prevent double-starting
            startRecognition();
        }

        // --- AI Processing (Gemini) ---
        async function processCommand(command) {
            // Signal that we are processing so onend doesn't auto-restart immediately
            isProcessing = true;
            
            try {
                // Stop recognition to prevent feedback loop (listening to its own speech)
                if (recognition && isListening) recognition.stop();
            } catch (e) {
                // Ignore stop errors, the recognition engine handles state changes messily
            }
            
            try {
                const response = await callGemini(command);
                await speak(response);
            } catch (error) {
                console.error("AI Error:", error);
                await speak("I'm unable to process that request at the moment.");
            } finally {
                // 1. Mark as no longer processing
                isProcessing = false;
                
                // 2. Since TTS is finished (due to await speak()), restart listening immediately.
                startRecognition();
            }
        }

        async function callGemini(prompt) {
            // Using exponential backoff for API calls
            const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`;
            
            // --- UPDATED SYSTEM PROMPT FOR CONCISENESS AND DIRECTNESS ---
            const systemPrompt = "You are a direct, highly accurate, and concise AI assistant named Jarvis. Provide only the factual answer to the user's query, without any introductory, conversational, or concluding remarks. Your response must be suitable for being spoken out loud and must be in plain text.";
            
            // Prepare the current user turn
            const userTurn = {
                role: "user",
                parts: [{ text: prompt }]
            };

            // Send the entire chat history plus the new user turn
            const contentsToSend = [...chatHistory, userTurn];

            const data = {
                contents: contentsToSend, // Use the full conversation history
                systemInstruction: {
                    parts: [{ text: systemPrompt }]
                },
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 512, 
                }
            };

            const maxRetries = 3;
            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetch(url, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(data)
                    });

                    if (!response.ok) {
                        if (response.status === 429 && i < maxRetries - 1) { // 429 Too Many Requests
                            const delay = Math.pow(2, i) * 1000;
                            await new Promise(resolve => setTimeout(resolve, delay));
                            continue; // Retry
                        }
                        throw new Error(`HTTP error! status: ${response.status} - ${response.statusText}`);
                    }

                    const result = await response.json();
                    
                    if (result.candidates && result.candidates.length > 0 &&
                        result.candidates[0].content && result.candidates[0].content.parts &&
                        result.candidates[0].content.parts.length > 0 &&
                        result.candidates[0].content.parts[0].text) {
                        
                        const generatedText = result.candidates[0].content.parts[0].text;
                        
                        // ADD THE CONVERSATION TO HISTORY
                        chatHistory.push(userTurn);
                        chatHistory.push({ role: "model", parts: [{ text: generatedText }] });

                        return generatedText;
                    } else {
                        console.error("Gemini API returned unexpected structure or no content:", result);
                        throw new Error("API response was missing expected generated content.");
                    }

                } catch (error) {
                    if (i === maxRetries - 1) {
                        throw error; // Re-throw if it's the last attempt
                    }
                    const delay = Math.pow(2, i) * 1000;
                    await new Promise(resolve => setTimeout(resolve, delay));
                }
            }
            return "An unexpected error occurred.";
        }

        // --- Text to Speech ---
        function loadVoices() {
            voices = synthesis.getVoices();
            // Try to load voices again if empty (browser quirk)
            if (voices.length === 0) {
                synthesis.onvoiceschanged = () => {
                    voices = synthesis.getVoices();
                };
            }
        }

        function speak(text) {
            return new Promise((resolve) => {
                if (synthesis.speaking) synthesis.cancel();

                isSpeaking = true;
                setVisualState('speaking');
                setStatus("Speaking...");

                const utterance = new SpeechSynthesisUtterance(text);
                
                // Voice selection logic
                const preferredVoice = voices.find(voice => 
                    voice.name.includes('Google UK English Male') || 
                    voice.name.includes('Google US English') || 
                    voice.name.includes('Daniel')
                );
                
                if (preferredVoice) utterance.voice = preferredVoice;
                
                utterance.pitch = 0.9;
                utterance.rate = 1.0;
                utterance.volume = 1.0;

                utterance.onend = () => {
                    isSpeaking = false;
                    setVisualState('idle');
                    setStatus("Standing By");
                    resolve();
                    // RESTART LOGIC IS HANDLED BY processCommand'S FINALLY BLOCK
                };

                utterance.onerror = (e) => {
                    console.error("TTS Error", e);
                    isSpeaking = false;
                    setVisualState('idle');
                    resolve();
                };

                synthesis.speak(utterance);
            });
        }

        // --- UI Helpers ---
        function setStatus(text) {
            statusEl.textContent = text;
        }

        function setVisualState(state) {
            // states: 'idle', 'active', 'speaking'
            bodyEl.className = ''; // clear classes
            bodyEl.classList.add(`state-${state}`);
            
            if (state === 'idle') {
                timeEl.style.opacity = '1';
                dateEl.style.opacity = '0.8';
            }
        }

    </script>
</body>
</html>
